<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>mpickering.github.io</title>
    <link href="http://mpickering.github.io/atom.xml" rel="self" />
    <link href="http://mpickering.github.io" />
    <id>http://mpickering.github.io/atom.xml</id>
    <author>
        <name>Matthew Pickering</name>
        <email>matthewtpickering@gmail.com</email>
    </author>
    <updated>2019-11-07T00:00:00Z</updated>
    <entry>
    <title>Introducing hs-speedscope - a way to visualise time profiles</title>
    <link href="http://mpickering.github.io/posts/2019-11-07-hs-speedscope.html" />
    <id>http://mpickering.github.io/posts/2019-11-07-hs-speedscope.html</id>
    <published>2019-11-07T00:00:00Z</published>
    <updated>2019-11-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Introducing hs-speedscope - a way to visualise time profiles </h2>
<p class="text-muted">
    Posted on November  7, 2019
    
</p>

<p>In GHC-8.10 it will become possible to use <a href="https://www.speedscope.app/">speedscope</a> to visualise the performance of a Haskell program. speedscope is an interactive flamegraph visualiser, we can use it to visualise the output of the <code>-p</code> profiling option. Here’s how to use it:</p>
<ol type="1">
<li>Run your program with <code>prog +RTS -p -l-au</code>. This will create an eventlog with cost centre stack sample events.</li>
<li>Convert the eventlog into the speedscope JSON format using <a href="https://github.com/mpickering/hs-speedscope"><code>hs-speedscope</code></a>. The <code>hs-speedscope</code> executable takes an eventlog file as the input and produces the speedscope JSON file.</li>
<li>Load the resulting <code>prog.eventlog.json</code> file into <a href="http://www.speedscope.app">speedscope.app</a>.</li>
</ol>
<!--more-->
<h2 id="using-speedscope">Using Speedscope</h2>
<p>Speedscope then has three modes for viewing the profile. The default mode shows you the executation trace of your program. The call stack extends downwards, the wider the box, the more time was spent in that part of the program. You can select part of the profile by selecting part of the minimap, zoom using <code>+/-</code> or pan using the arrow keys. The follow examples are from profiling GHC building Cabal:</p>
<p><a href="/images/speedscope1.png"><img src="/images/speedscope1.png" style="width:100.0%" /></a></p>
<p>The first summarised view is accessed by the “left-heavy” tab. This is like the summarised output of the <code>-p</code> flag. The cost centre stacks which account for the most time will be grouped together at the left of the view. This way you can easily see which executation paths take the longest over the course of the whole executation of the program.</p>
<p><a href="/images/speedscope2.png"><img src="/images/speedscope2.png" style="width:100.0%" /></a></p>
<p>Finally, the “sandwich” view tries to work out which specific cost centre is responsible for the most executation time. You can use this to try to understand the functions which take the most time to execute. How useful this view is depends on the resolution of your cost centres in your program. Speedscope attempts to work out the most expensive cost centre by subtracting the total time spent beneath that cost centre from the time spent in the cost centre. For example, if <code>f</code> calls <code>g</code> and <code>h</code>, the cost of in <code>f</code> is calculated by the total time for <code>f</code> minus the time spend in <code>g</code> and <code>h</code>. If the cost of <code>f</code> is high, then there is some computation happening in <code>f</code> which is not captured by any further cost centres.</p>
<p><a href="/images/speedscope3.png"><img src="../images/speedscope3.png" style="width:100.0%" /></a></p>
<h2 id="how-is-this-different-to-the-other-profile-visualisers">How is this different to the other profile visualisers?</h2>
<p>The most important difference is that I didn’t implement the visualiser, it is a generic visualiser which can support many different languages. I don’t have to maintain the visualiser or work out how to make it scale to very big profiles. You can easily load 60mb profiles using speedscope without any rendering problems. All the library does is directly convert the eventlog into the generic speedscope JSON format.</p>
<h2 id="how-is-this-different-to-the-support-already-in-speedscope">How is this different to the support already in speedscope?</h2>
<p>If you consult the documentation for speedscope you will see that it claims to support Haskell programs already. Rudimentary support has already been implemented by using the JSON output produced by the <code>-pj</code> flag but the default view which shows an executation trace of your program hasn’t worked correctly because the output of <code>-pj</code> is too generalised. If you program ends up calling the same code path many different times during the executation of the program, they are all identified in the final profile.</p>
<p>The second important difference is that each capability will be displayed on a separate profile. This makes profiling more useful for parallel programs.</p>
<h2 id="how-does-it-work">How does it work?</h2>
<p>I <a href="https://gitlab.haskell.org/ghc/ghc/merge_requests/1927">added support</a> to dump the raw output from <code>-p</code> to the eventlog. Now it’s possible to process the raw information in order to produce the format that speedscope requires.</p>
<h2 id="additional-links">Additional Links</h2>
<ul>
<li><a href="https://www.reddit.com/r/haskell/comments/dt9acz/introducing_hsspeedscope/">Reddit</a></li>
<li><a href="https://github.com/mpickering/hs-speedscope"><code>hs-speedscope</code></a></li>
</ul>
]]></summary>
</entry>
<entry>
    <title>Announcing Bristol Haskell Hackathon 2020</title>
    <link href="http://mpickering.github.io/posts/2019-10-21-bristol-haskell-2020.html" />
    <id>http://mpickering.github.io/posts/2019-10-21-bristol-haskell-2020.html</id>
    <published>2019-10-21T00:00:00Z</published>
    <updated>2019-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Announcing Bristol Haskell Hackathon 2020 </h2>
<p class="text-muted">
    Posted on October 21, 2019
    
</p>

<p>I have decided to organise an informal hackathon in Bristol at the start of next year.</p>
<div class="table">
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">When</td>
<td style="text-align: left;">25th-26th January 2020</td>
</tr>
<tr class="even">
<td style="text-align: left;">Where</td>
<td style="text-align: left;"><a href="https://goo.gl/maps/x3q61a3zbyTfc7ZH6">Merchant Venturers Building - University of Bristol</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Time</td>
<td style="text-align: left;">09:00 - 17:00</td>
</tr>
</tbody>
</table>
</div>
<p>Anyone interested in Haskell is welcome to attend. Whether you are a beginner or an expert it would be great to meet you in Bristol.</p>
<p>It is a no-frills hackathon, we’ll provide a room for hacking and wifi but expect little else! There will be no t-shirts, food, talks or other perks. The focus will be 100% on hacking and meeting other Haskell programmers.</p>
<p>For further information about the event and how to register please refer to the dedicated page.</p>
<div class="text-center">
<p><a href="../bristol2020.html"><button type="button" class="btn btn-secondary">More information about Bristol 2020</button></a></p>
</div>
]]></summary>
</entry>
<entry>
    <title>Two new Haskell Symposium papers</title>
    <link href="http://mpickering.github.io/posts/2019-07-09-haskell-papers.html" />
    <id>http://mpickering.github.io/posts/2019-07-09-haskell-papers.html</id>
    <published>2019-07-09T00:00:00Z</published>
    <updated>2019-07-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Two new Haskell Symposium papers </h2>
<p class="text-muted">
    Posted on July  9, 2019
    
</p>

<p>This year I was lucky to have both my papers accepted for the Haskell Symposium. The first one is about the problematic interaction of Typed Template Haskell and implicit arguments and the second, a guide to writing source plugins. Read on the abstracts and download links.</p>
<!--more-->
<h3 id="multi-stage-programming-in-context">Multi Stage Programming in Context</h3>
<p>Matthew Pickering, Nicolas Wu, Csongor Kiss (<a href="../papers/multi-stage-programs-in-context.pdf">PDF</a>)</p>
<div class="blockquote">
<blockquote>
<p>Cross-stage persistence is an essential aspect of multi-stage programming that allows a value defined in one stage to be available in another. However, difficulty arises when implicit information held in types, type classes and implicit parameters needs to be persisted. Without a careful treatment of such implicit information—which are pervasive in Haskell—subtle yet avoidable bugs lurk beneath the surface.</p>
</blockquote>
<blockquote>
<p>This paper demonstrates that in multi-stage programming care must be taken when representing quoted terms so that important implicit information is not discarded. The approach is formalised with a type-system, and an implementation in GHC is presented that fixes problems of the previous incarnation.</p>
</blockquote>
</div>
<h3 id="working-with-source-plugins">Working with Source Plugins</h3>
<p>Matthew Pickering, Nicolas Wu, Boldizsár Németh (<a href="../papers/working-with-source-plugins.pdf">PDF</a>)</p>
<div class="blockquote">
<blockquote>
<p>A modern compiler calculates and constructs a large amount of information about the programs it compiles. Tooling authors want to take advantage of this information in order to extend the compiler in interesting ways. Source plugins are a mechanism implemented in the Glasgow Haskell Compiler (GHC) which allow inspection and modification of programs as they pass through the compilation pipeline.</p>
</blockquote>
<blockquote>
<p>This paper is about how to write source plugins. Due to their nature–they are ways to extend the compiler–at least basic knowledge about how the compiler works is critical to designing and implementing a robust and therefore successful plugin. The goal of the paper is to equip would-be plugin authors with inspiration about what kinds of plugins they should write and most importantly with the basic techniques which should be used in order to write them.</p>
</blockquote>
</div>
]]></summary>
</entry>
<entry>
    <title>Complete overkill or exactly right? Deploying a static site using nix</title>
    <link href="http://mpickering.github.io/posts/2019-06-24-overkill-or-not.html" />
    <id>http://mpickering.github.io/posts/2019-06-24-overkill-or-not.html</id>
    <published>2019-06-24T00:00:00Z</published>
    <updated>2019-06-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Complete overkill or exactly right? Deploying a static site using nix </h2>
<p class="text-muted">
    Posted on June 24, 2019
    
</p>

<p><a href="https://mpickering.github.io/eventlog2html/"><code>eventlog2html</code></a> is my new library for visualising Haskell heap profiles as an interactive webpage.</p>
<p>For the documentation, I thought it was important to provide some interactive examples which is why I decided to host my own static webpage rather than rely on a GitHub README. This led to two constraints:</p>
<ol type="1">
<li>The documentation is a static web page containing up-to-date examples of the tool’s output.</li>
<li>The page should be automatically deployed using CI.</li>
</ol>
<p>This post is a question about whether the combination of <a href="https://nixos.org/nix/">nix</a>, <a href="https://cachix.org/">Cachix</a>, <a href="https://travis-ci.org/">Travis CI</a>, <a href="https://input-output-hk.github.io/haskell.nix/">haskell.nix</a> and <a href="https://jaspervdj.be/hakyll/">Hakyll</a> was the perfect solution to these constraints or an exercise in overkill.</p>
<!--more-->
<h1 id="generating-the-static-site">Generating the static site</h1>
<p>The static site is generated using Hakyll. The content is written using markdown and rendered using pandoc. Inline charts are specified using special code blocks.</p>
<pre><code>```{.eventlog traces=False }
examples/ghc.eventlog --bands 10
```</code></pre>
<p>A <a href="https://pandoc.org/filters.html">pandoc filter</a> identifiers a code block which has the <code>eventlog</code> class and replaces it with the suitable visualisation. Options can be specified as attributes or using normal command line arguments.</p>
<p>Using a site generator implemented in Haskell meant that I could import <code>eventlog2html</code> as a library and use it directly without having to modify the external interface. This ended up being about <a href="https://github.com/mpickering/eventlog2html/blob/master/hakyll-eventlog/site.hs#L87">40 lines</a> for the filter which inserts eventlogs. There is also a <a href="https://github.com/mpickering/eventlog2html/blob/master/hakyll-eventlog/site.hs#L142">simpler filter</a> which inserts the result of calling <code>--help</code>.</p>
<p>Using Hakyll has already proved to be a good idea when I wanted to add the <a href="https://mpickering.github.io/eventlog2html/examples.html">examples gallery</a>. It was trivial to generate this page from a folder of eventlogs so that all I have to do to add a new eventlog is commit it to the repo.</p>
<p>So far, I haven’t broken the complexity budget. In order to satisfy the first constraint and keep the generated documentation up to date I created a package for the site. In the <code>cabal.project</code> file I then added the site’s folder as a subdirectory. Now, <code>hakyll-eventlog</code> will use the local version of <code>eventlog2html</code> as a dependency when it builds the site.</p>
<pre><code>packages: .
          hakyll-eventlog</code></pre>
<p>The site can be built and run using <code>cabal new-build hakyll-eventlog</code>. Now we move onto how to perform deployment of the generated site.</p>
<h1 id="deploying-using-travis">Deploying using Travis</h1>
<p>CircleCI and Travis are both popular CI providers and they can both to deploy to GitHub Pages. However, the Travis integration was far simpler to set up. There is built-in support for GitHub pages as a deployment target so a single stanza is necessary to perform the deployment.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="fu">deploy:</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">  <span class="fu">provider:</span><span class="at"> pages</span></a>
<a class="sourceLine" id="cb3-3" data-line-number="3">  <span class="fu">skip_cleanup:</span><span class="at"> true</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">  <span class="fu">github_token:</span><span class="at"> $GITHUB_TOKEN</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">  <span class="fu">keep_history:</span><span class="at"> true</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">  <span class="fu">target_branch:</span><span class="at"> gh-pages</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">  <span class="fu">local_dir:</span><span class="at"> site</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8">  <span class="fu">on:</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9">    <span class="fu">tags:</span><span class="at"> true</span></a></code></pre></div>
<p>The stanza says, deploy to GitHub pages by pushing the contents of the <code>site</code> directory to the <code>gh-pages</code> branch of the current repository. GitHub then serves the contents of the <code>gh-pages</code> branch on <code>https://mpickering.github.io/eventlog2html</code>.</p>
<p>Now all we need to do is generate the <code>site</code> directory. I found it quite daunting to modify the Travis script generated by <a href="http://hackage.haskell.org/package/haskell-ci"><code>haskell-ci</code></a> so at this point I decided to convert all the CI infrastructure to use nix instead.</p>
<h1 id="building-using-nix">Building using nix</h1>
<p>An obvious question at this stage is why is nix necessary at all? Wouldn’t a CI configuration which uses cabal have worked equally as well? On reflection, I could think of four reasons why I considered this to be a good idea.</p>
<ol type="1">
<li>Much more concise than the <code>haskell-ci</code> generated travis file.</li>
<li>Easier to run the same script locally</li>
<li>Easier for other nix users to use the project</li>
<li>Easy caching with Cachix</li>
</ol>
<h2 id="haskell.nix"><code>haskell.nix</code></h2>
<p>A key part in the decision was the new <a href="https://github.com/input-output-hk/haskell.nix"><code>haskell.nix</code></a> tooling to build Haskell packages. If you use the normal Haskell infrastructure which is built into nixpkgs then any collaborator has to know about nix in order to fix CI when it breaks. On the other hand, <code>haskell.nix</code> creates its derivations from the result of <code>cabal new-configure</code> so it matches up with using a <code>new-build</code> workflow locally.</p>
<p>Purity is retained by explicitly passing the <code>--index-state</code> flag to <code>new-configure</code> so anyone can update the CI configuration by changing the index state parameter in the <code>default.nix</code> file.</p>
<p>How does this look in practice? The <code>default.nix</code> is a very concise script which calls <code>haskell.nix</code>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode nix"><code class="sourceCode bash"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="bu">let</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">  <span class="ex">pin</span> = import ((import ./nix/sources.nix)<span class="ex">.nixpkgs</span>) <span class="dt">{}</span> ;</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">  <span class="co"># Import the Haskell.nix library,</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">  <span class="ex">haskell</span> = import (builtins.fetchTarball https://github.com/input-output-hk/haskell.nix/archive/master.tar.gz) <span class="kw">{</span> <span class="ex">pkgs</span> = pin<span class="kw">;</span> <span class="kw">}</span>;</a>
<a class="sourceLine" id="cb4-6" data-line-number="6"></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">  <span class="co"># Generate the pkgs.nix file using callCabalProjectToNix IFD</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">  <span class="ex">pkgPlan</span> = haskell.callCabalProjectToNix</a>
<a class="sourceLine" id="cb4-9" data-line-number="9">              <span class="kw">{</span> <span class="ex">index-state</span> = <span class="st">&quot;2019-05-10T00:00:00Z&quot;</span></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">              ; <span class="ex">src</span> = pin.lib.cleanSource ./.<span class="kw">;}</span>;</a>
<a class="sourceLine" id="cb4-11" data-line-number="11"></a>
<a class="sourceLine" id="cb4-12" data-line-number="12">  <span class="co"># Instantiate a package set using the generated file.</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13">  <span class="ex">pkgSet</span> = haskell.mkCabalProjectPkgSet {</a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    <span class="ex">plan-pkgs</span> = import pkgPlan<span class="kw">;</span></a>
<a class="sourceLine" id="cb4-15" data-line-number="15">    <span class="ex">pkg-def-extras</span> = []<span class="kw">;</span></a>
<a class="sourceLine" id="cb4-16" data-line-number="16">    <span class="ex">modules</span> = []<span class="kw">;</span></a>
<a class="sourceLine" id="cb4-17" data-line-number="17">  };</a>
<a class="sourceLine" id="cb4-18" data-line-number="18"></a>
<a class="sourceLine" id="cb4-19" data-line-number="19">  <span class="ex">site</span> = import ./nix/site.nix { nixpkgs = pin<span class="kw">;</span> <span class="ex">hspkgs</span> = pkgSet.config.hsPkgs<span class="kw">;</span> };</a>
<a class="sourceLine" id="cb4-20" data-line-number="20"></a>
<a class="sourceLine" id="cb4-21" data-line-number="21"><span class="kw">in</span></a>
<a class="sourceLine" id="cb4-22" data-line-number="22">  <span class="kw">{</span> <span class="ex">eventlog2html</span> = pkgSet.config.hsPkgs.eventlog2html.components.exes.eventlog2html <span class="kw">;</span></a>
<a class="sourceLine" id="cb4-23" data-line-number="23">    <span class="ex">site</span> = site<span class="kw">;</span> <span class="kw">}</span></a></code></pre></div>
<p>The <code>callCabalProjectToNix</code> function is the key. That is the function which calls <code>new-configure</code> to create the build plan directly using cabal. It produces the same result as calling <code>plan-to-json</code> manually, as <a href="https://input-output-hk.github.io/haskell.nix/user-guide/cabal-projects/">the documentation</a> explains how you should use <code>haskell.nix</code>. Therefore, the rest of the documentation can be followed but with the difference that the result of <code>callCabalProjectToNix</code> is passed as an argument to <code>mkCabalProjectPkgSet</code> rather than an explicit <code>pkgs.nix</code> file.</p>
<p>A derivation which generates the documentation site is also created. The definition is simple because <code>haskell.nix</code> takes care of building the site generator for us. All the derivation does it apply the site generator to the contents of the <code>docs/</code> subdirectory.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode nix"><code class="sourceCode bash"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">{</span> <span class="ex">nixpkgs</span>, hspkgs <span class="kw">}</span>:</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="ex">nixpkgs.stdenv.mkDerivation</span> {</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">  <span class="ex">name</span> = <span class="st">&quot;docs-0.1&quot;</span><span class="kw">;</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"></a>
<a class="sourceLine" id="cb5-5" data-line-number="5">  <span class="ex">src</span> = nixpkgs.lib.cleanSource ../docs<span class="kw">;</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">  <span class="ex">LANG</span> = <span class="st">&quot;en_US.UTF-8&quot;</span><span class="kw">;</span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7">  <span class="ex">LOCALE_ARCHIVE</span> = <span class="st">&quot;</span><span class="va">${nixpkgs</span><span class="er">.glibcLocales</span><span class="va">}</span><span class="st">/lib/locale/locale-archive&quot;</span><span class="kw">;</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8"></a>
<a class="sourceLine" id="cb5-9" data-line-number="9">  <span class="ex">buildInputs</span> = [ hspkgs.hakyll-eventlog.components.exes.site ]<span class="kw">;</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10"></a>
<a class="sourceLine" id="cb5-11" data-line-number="11">  <span class="ex">preConfigure</span> = <span class="st">&#39;&#39;</span>export LANG=<span class="st">&quot;en_US.UTF-8&quot;</span><span class="kw">;</span><span class="st">&#39;&#39;</span>;</a>
<a class="sourceLine" id="cb5-12" data-line-number="12"></a>
<a class="sourceLine" id="cb5-13" data-line-number="13">  <span class="ex">buildPhase</span> = <span class="st">&#39;&#39;</span>site build<span class="st">&#39;&#39;</span><span class="kw">;</span></a>
<a class="sourceLine" id="cb5-14" data-line-number="14"></a>
<a class="sourceLine" id="cb5-15" data-line-number="15">  <span class="ex">installPhase</span> = <span class="st">&#39;&#39;</span>cp -r _site <span class="va">$out</span><span class="st">&#39;&#39;</span><span class="kw">;</span></a>
<a class="sourceLine" id="cb5-16" data-line-number="16">}</a></code></pre></div>
<p>Evaluating <code>default.nix</code> results in the a set containing the two outputs of the project. The executable <code>eventlog2html</code> and the documentation site. You can build each attribute locally</p>
<pre><code>cachix use mpickering
nix build -f . eventlog2html
nix build -f . site</code></pre>
<p>but also by passing a link to the generated github tarball.</p>
<pre><code>nix run -f https://github.com/mpickering/eventlog2html/archive/master.tar.gz eventlog2html -c eventlog2html my-leaky-program.eventlog</code></pre>
<h2 id="updated-travis-configuration">Updated Travis configuration</h2>
<p>The build job now calls nix to build these scripts and uses the <code>-o</code> flag to place the output into the <code>site</code> directory. The precise location where Travis expected to find the generated site so the deployment step can now find the files.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">-</span> <span class="fu">stage:</span><span class="at"> build documentation</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2">    <span class="fu">script:</span></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">      <span class="kw">-</span> <span class="fu">nix-env -iA cachix -f https:</span><span class="at">//cachix.org/api/v1/install</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4">      <span class="kw">-</span> cachix use mpickering</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">      <span class="kw">-</span> cachix push mpickering --watch-store&amp;</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">      <span class="kw">-</span> nix-build -A site -o site</a></code></pre></div>
<p>We use Cachix to cache the result of building the individual derivations. This makes a huge difference to the total time that CI takes to run.</p>
<div class="alert alert-info" data-role="alert">
<p>You can greatly speed up the initial CI runs by pushing local build artifacts to travis.</p>
<pre><code>nix-store -qR --include-outputs $(nix-instantiate default.nix) | cachix push mpickering</code></pre>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>That’s basically it. Despite a complicated amalgamation of tools, everything worked out nicely together without any horrible hacks. All I had to do was to work out how to fix the pieces together. When using bleeding edge technology such as <code>haskell.nix</code>, this isn’t always straightforward but now I’ve documented my struggles the next person should find it easier.</p>
<h2 id="addendum-using-secure-env-vars-in-travis">Addendum: Using secure env vars in Travis</h2>
<p>We need to set two env vars for CI to work. You have to encrypt these so you can place them into the public <code>.travis.yml</code> file without exposing secrets.</p>
<ul>
<li><code>GITHUB_TOKEN</code> - To allow travis to push to the repo</li>
<li><code>CACHIX_SIGNING_KEY</code> - To allow Cachix to push to a cache</li>
</ul>
<p>To generate the <code>GITHUB_TOKEN</code> go to <a href="https://github.com/settings/tokens">GitHub settings</a> and generate a token with the <code>public_repo</code> permissions.</p>
<p>The <code>CACHIX_SIGNING_KEY</code> can be found in <code>~/.config/cachix/cachix.dhall</code> in the <code>secreyKey</code> field for the corresponding binary cache.</p>
<p>Once you have the keys you have to encrypt them using the <code>travis</code> command line application.</p>
<pre><code>nix-shell -p travis
travis encrypt GITHUB_TOKEN=token
travis encrypt CACHIX_SIGNING_KEY=token</code></pre>
<p>Then copy and paste the result into your <code>.travis.yml</code> file. Make sure you add the <code>-</code> so the field is treated as a list. Otherwise Travis will ignore one of your keys.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="fu">env:</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2">  <span class="fu">global:</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3">    <span class="co"># github</span></a>
<a class="sourceLine" id="cb11-4" data-line-number="4">    <span class="kw">-</span> <span class="fu">secure:</span><span class="at"> &lt;enrypted-key-1&gt;</span></a>
<a class="sourceLine" id="cb11-5" data-line-number="5"></a>
<a class="sourceLine" id="cb11-6" data-line-number="6">    <span class="co"># cachix</span></a>
<a class="sourceLine" id="cb11-7" data-line-number="7">    <span class="kw">-</span> <span class="fu">secure:</span><span class="at"> &lt;encrypted-key-2&gt;</span></a></code></pre></div>
]]></summary>
</entry>
<entry>
    <title>Tools for working on GHC</title>
    <link href="http://mpickering.github.io/posts/2019-06-11-ghc-tools.html" />
    <id>http://mpickering.github.io/posts/2019-06-11-ghc-tools.html</id>
    <published>2019-06-11T00:00:00Z</published>
    <updated>2019-06-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Tools for working on GHC </h2>
<p class="text-muted">
    Posted on June 11, 2019
    
</p>

<p>In the old days of the Make build system, the only reliable IDE-like feature which was useful whilst working on GHC was a tags file. Even loading GHC into GHCi was not easily possible, the most simple of interactive development workflows. Thankfully now times are changing, there are now build targets to start a GHCi session which enables developers to use tooling such as <a href="https://github.com/ndmitchell/ghcid">ghcid</a> or <a href="https://marketplace.visualstudio.com/items?itemName=dramforever.vscode-ghc-simple">vscode-ghc-simple</a>. Something which is quite important when working on a project with over 500 modules!</p>
<p>In this post we’ll briefly describe some recent advancements in developer tooling which have been made possible by the move to Hadrian.</p>
<!--more-->
<h2 id="ghci"><code>ghci</code></h2>
<p>The first target allows a developer to load GHC into GHCi. The <code>-fno-code</code> option is used which means that you can’t evaluate any expressions. It is useful for rapid feedback.</p>
<script id="asciicast-EKHiPuGgxhXz0ZHQgtR3OQd9G" src="https://asciinema.org/a/EKHiPuGgxhXz0ZHQgtR3OQd9G.js" async></script>
<h2 id="ghcid"><code>ghcid</code></h2>
<p><code>ghcid</code> can be used whilst working on <code>ghc</code> by invoking the <code>./hadrian/ghci.sh</code> target.</p>
<script id="asciicast-HAu0U5cVbneuujaoUA92Nxld5" src="https://asciinema.org/a/HAu0U5cVbneuujaoUA92Nxld5.js" async></script>
<p>There is a <code>.ghcid</code> file included <a href="https://gitlab.haskell.org/ghc/ghc/blob/master/.ghcid">in the repo</a> which includes some basic settings instructing <code>.ghcid</code> to reload the session if <code>hadrian/</code> changes. It might also be useful to add further directories here so that working with the many components of <code>ghc</code> is seamless.</p>
<h2 id="haskell-ide-engine"><code>haskell-ide-engine</code></h2>
<p>Once you have a working <code>ghci</code> target then in theory it becomes possible to use all other tooling with your build system. I realised that it would be possible to get <code>haskell-ide-engine</code> working with <code>ghc</code> but it required a <a href="https://github.com/haskell/haskell-ide-engine/pull/1126">very significant refactor</a>.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
Here's a short demo of using haskell-ide-engine on GHC's code base using my fork which integrates HIE into hadrian/cabal/rules_haskell/stack/obelisk <a href="https://t.co/rA1ps7dSb1">pic.twitter.com/rA1ps7dSb1</a>
</p>
— Matthew Pickering (<span class="citation" data-cites="mpickering_">(<span class="citeproc-not-found" data-reference-id="mpickering_"><strong>???</strong></span>)</span>) <a href="https://twitter.com/mpickering_/status/1110874588509016064?ref_src=twsrc%5Etfw">March 27, 2019</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>As a result, the branch can’t easily be merged back into the main repo but once it is merged then <code>haskell-ide-engine</code> will be more flexible and target agnostic.</p>
<h2 id="future-work-running-main">Future work: running <code>:main</code></h2>
<p>A final <a href="https://gitlab.haskell.org/ghc/ghc/issues/16672">goal</a> is to be able to run GHC’s <code>main</code> function from inside the interpreter. In order to do this it’s necessary to interpret the code rather than pass <code>-fno-code</code>. With some modifications to the <code>./hadrian/ghci.sh</code> script and patches by Michael Sloan we have been able to load load <code>ghc</code> into <code>GHCi</code> in the interpreted mode.</p>
<p>Unfortunately, this isn’t enough as in order to build programs with <code>HEAD</code> you also need to build libraries such as <code>base</code> with <code>HEAD</code>. The way around this is to first compile stage2 and then use the stage2 compiler to launch GHCi and load GHC into that. Then the libraries will be the correct versions and can be used to compile other modules.</p>
<p>A few months ago I got this working but since then it seems that the workflow <a href="https://gitlab.haskell.org/ghc/ghc/issues/16797">has been broken</a>. It’s a bit unfortunate that you have to jump through so many hoops in order to compile even a simple module but this is a unavoidable consequence of how GHC compiles and uses modules.</p>
<h3 id="ghci-debugger">GHCi Debugger</h3>
<p>Once you can execute <code>:main</code>, you can also use the GHCi debugger to debug GHC itself! This works without any problems but until you can use <code>:main</code> to compile programs then its of limited utility. I used the debugger to find the original reason why <code>:main</code> was failing whe compiling a program.</p>
]]></summary>
</entry>
<entry>
    <title>Making use of GHC bindists built by GitLab CI</title>
    <link href="http://mpickering.github.io/posts/2019-06-11-ghc-artefact.html" />
    <id>http://mpickering.github.io/posts/2019-06-11-ghc-artefact.html</id>
    <published>2019-06-11T00:00:00Z</published>
    <updated>2019-06-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Making use of GHC bindists built by GitLab CI </h2>
<p class="text-muted">
    Posted on June 11, 2019
    
</p>

<p>The new GHC GitLab CI infrastructure builds hundreds of different commits a week. Each commit on <code>master</code> is built, as well as any merge requests; each build produces an bindist which can be downloaded and installed on the relevant platform.</p>
<p><a href="https://github.com/mpickering/ghc-artefact-nix"><code>ghc-artefact-nix</code></a> provides a program <code>ghc-head-from</code> which downloads and enters a shell providing an artefact built with GitLab CI.</p>
<!--more-->
<h2 id="using-ghc-artefact-nix">Using <code>ghc-artefact-nix</code></h2>
<p>You can install <code>ghc-head-from</code> using <a href="https://github.com/nix-community/NUR"><code>NUR</code></a>.</p>
<pre><code>nix-shell -p nur.repos.mpickering.ghc-head-from</code></pre>
<p>There are three modes of operation.</p>
<h3 id="grab-a-recent-commit-from-master">Grab a recent commit from <code>master</code></h3>
<pre><code>ghc-head-from</code></pre>
<h3 id="grab-a-merge-request">Grab a merge request</h3>
<pre><code>ghc-head-from 1107</code></pre>
<h3 id="grab-a-specific-bindist-for-example-from-a-branch-or-fork">Grab a specific bindist (for example, from a branch or fork)</h3>
<pre><code>ghc-head-from https://gitlab.haskell.org/ghc/ghc/-/jobs/98842/artifacts/raw/ghc-x86_64-fedora27-linux.tar.xz</code></pre>
<p>The URL you provide has to be a direct link to a <code>fedora27</code> bindist.</p>
<h2 id="technical-details">Technical Details</h2>
<p>The bindist is downloaded from the (very flaky) CDN and patched to remove platform specific paths. The <code>fedora27</code> job is used because it is built using <code>ncurses6</code> which works better with nix.</p>
<h3 id="using-an-artefact-in-a-nix-expression">Using an artefact in a nix expression</h3>
<p>The <a href="https://github.com/mpickering/old-ghc-nix"><code>old-ghc-nix</code></a> repo provides a <code>mkGhc</code> function which can be used in a nix expression to create an attribute for a specific bindist. It is also packaged using <code>NUR</code>.</p>
<pre><code>nur.repos.mpickering.ghc.mkGhc
  {  url = &quot;https://gitlab-artifact-url.com&quot;; hash = &quot;sha256&quot;; ncursesVersion = &quot;6&quot;; }</code></pre>
<p>The <code>ncursesVersion</code> attribute is important to set for <code>fedora27</code> jobs as the function assumes that the bindist was built with <code>deb8</code> which uses <code>ncurses5</code>.</p>
<p>If you plan on using the artefact for a while then make sure you click the “keep” button on the artefact download page as otherwise it will be deleted after a week. This is very useful if you are developing a library against an unreleased version of the compiler and want to make sure all your collaborators are using the same version of GHC.</p>
]]></summary>
</entry>
<entry>
    <title>A three-stage program you definitely want to write</title>
    <link href="http://mpickering.github.io/posts/2019-02-14-stage-3.html" />
    <id>http://mpickering.github.io/posts/2019-02-14-stage-3.html</id>
    <published>2019-02-14T00:00:00Z</published>
    <updated>2019-02-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> A three-stage program you definitely want to write </h2>
<p class="text-muted">
    Posted on February 14, 2019
    
</p>

<p>Writing programs explicitly in stages gives you guarantees that abstraction will be removed. A guarantee that the optimiser most certainly does not give you.</p>
<p>After spending the majority of my early 20s inside the optimiser, I decided enough was enough and it was time to gain back control over how my programs were partially evaluated.</p>
<p>So in this post I’ll give an example of how I took back control and eliminated two levels of abstraction for an interpreter by writing a program which runs in three stages.</p>
<p>Enter: An applicative interpreter for Hutton’s razor.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">data</span> <span class="dt">Expr</span> <span class="fu">=</span> <span class="dt">Val</span> <span class="dt">Int</span> <span class="fu">|</span> <span class="dt">Add</span> <span class="dt">Expr</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="ot">eval ::</span> <span class="dt">Applicative</span> m <span class="ot">=&gt;</span> <span class="dt">Expr</span> <span class="ot">-&gt;</span> m <span class="dt">Int</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">eval (<span class="dt">Val</span> n) <span class="fu">=</span> pure n</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">eval (<span class="dt">Add</span> e1 e2) <span class="fu">=</span> (<span class="fu">+</span>) <span class="fu">&lt;$&gt;</span> eval e1 <span class="fu">&lt;*&gt;</span> eval e2</a></code></pre></div>
<p>Written simply at one level, there are two levels of abstraction which could be failed to be eliminated.</p>
<ol type="1">
<li>If we statically know the expression we can eliminate <code>Expr</code>.</li>
<li>If we statically know which <code>Applicative</code> then we can remove the indirection from the typeclass.</li>
</ol>
<p>Using typed Template Haskell we’ll work out how to remove both of these layers.</p>
<!--more-->
<h2 id="eliminating-the-expression">Eliminating the Expression</h2>
<p>First we’ll have a look at how to stage the program just to eliminate the expression without discussion the application fragment. This is a two-stage program.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">module</span> <span class="dt">Two</span> <span class="kw">where</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw">import</span> <span class="dt">Language.Haskell.TH</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="kw">data</span> <span class="dt">Expr</span> <span class="fu">=</span> <span class="dt">Val</span> <span class="dt">Int</span> <span class="fu">|</span> <span class="dt">Add</span> <span class="dt">Expr</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="ot">eval ::</span> <span class="dt">Expr</span> <span class="ot">-&gt;</span> <span class="dt">TExpQ</span> <span class="dt">Int</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">eval (<span class="dt">Val</span> n) <span class="fu">=</span> [<span class="fu">||</span> n <span class="fu">||</span>]</a>
<a class="sourceLine" id="cb2-9" data-line-number="9">eval (<span class="dt">Add</span> e1 e2) <span class="fu">=</span> [<span class="fu">||</span> <span class="fu">$$</span>(eval e1) <span class="fu">+</span> <span class="fu">$$</span>(eval e2) <span class="fu">||</span>]</a></code></pre></div>
<p>The eval function takes an expression and generates code which unrolls the expression that needs to be evaluated.</p>
<p>Splicing in <code>eval</code> gives us a chain of additions which are computed at run-time.</p>
<pre><code>$$(eval (Add (Val 1) (Val 2)))
=&gt; 1 + 2</code></pre>
<p>By explicitly separating the program into stages we know that there will be no mention of <code>Expr</code> in the resulting program.</p>
<h2 id="eliminating-the-applicative-functor">Eliminating the Applicative Functor</h2>
<p>That’s good. Eliminating the <code>Expr</code> data type was easy. We’ll have to work a bit more to eliminate the applicative.</p>
<p>In the first stage, we will eliminate the expression in the same manner but instead of producing an <code>Int</code>, we will produce a <code>SynApplicative</code> which is a syntactic representation of an applicative. This allows us to inspect the structure of the program in the second stage and remove that overhead as well.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">data</span> <span class="dt">SynApplicative</span> a <span class="kw">where</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">  <span class="dt">Return</span><span class="ot"> ::</span> <span class="dt">WithCode</span> a <span class="ot">-&gt;</span> <span class="dt">SynApplicative</span> a</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">  <span class="dt">App</span><span class="ot">  ::</span> <span class="dt">SynApplicative</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> <span class="dt">SynApplicative</span> a <span class="ot">-&gt;</span> <span class="dt">SynApplicative</span> b</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="kw">data</span> <span class="dt">WithCode</span> a <span class="fu">=</span> <span class="dt">WithCode</span> {<span class="ot"> _val ::</span> a,<span class="ot"> _code ::</span> <span class="dt">TExpQ</span> a }</a></code></pre></div>
<p><code>WithCode</code> is a wrapper which pairs a value with a code fragment which was used to produce that value.</p>
<p>If you notice in the earlier example, this wasn’t necessary when it was known that we needed to persist an <code>Int</code>, as there is a <code>Lift</code> instance for <code>Int</code>. However, in general, not all values can be persisted so using <code>WithCode</code> is more general and flexible, if a bit more verbose.</p>
<p><code>elimExpr</code> eliminates the first layer of abstraction and returns code which generates a <code>SynApplicative</code>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="ot">elimExpr ::</span> <span class="dt">Expr</span> <span class="ot">-&gt;</span> <span class="dt">TExpQ</span> (<span class="dt">SynApplicative</span> <span class="dt">Int</span>)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">elimExpr (<span class="dt">Val</span> n) <span class="fu">=</span> [<span class="fu">||</span> <span class="dt">Return</span> (<span class="dt">WithCode</span> n (liftT n)) <span class="fu">||</span>]</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">elimExpr (<span class="dt">Add</span> e1 e2) <span class="fu">=</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">   [<span class="fu">||</span> <span class="dt">Return</span> (<span class="dt">WithCode</span> (<span class="fu">+</span>) codePlus)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">        <span class="ot">`App`</span> <span class="fu">$$</span>(elimExpr e1)</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">        <span class="ot">`App`</span> <span class="fu">$$</span>(elimExpr e2) <span class="fu">||</span>]</a>
<a class="sourceLine" id="cb5-7" data-line-number="7"></a>
<a class="sourceLine" id="cb5-8" data-line-number="8"><span class="ot">liftT ::</span> <span class="dt">Lift</span> a <span class="ot">=&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">TExpQ</span> a</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">liftT <span class="fu">=</span> unsafeTExpCoerce <span class="fu">.</span> lift</a>
<a class="sourceLine" id="cb5-10" data-line-number="10"></a>
<a class="sourceLine" id="cb5-11" data-line-number="11">codePlus <span class="fu">=</span> [<span class="fu">||</span> (<span class="fu">+</span>) <span class="fu">||</span>]</a></code></pre></div>
<p>In the case for <code>Add</code> we encounter a situation where we would have liked to use nested brackets to persist the value of <code>[|| (+) ||]</code>. Instead you have to lift it to the top level and then persist that identifier.</p>
<p>Next, it’s time to provide an interpreter to remove the abstraction of the applicative. In order to do this, we need to provide a dictionary which will be used to give the interpretation of the applicative commands.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">data</span> <span class="dt">ApplicativeDict</span> m <span class="fu">=</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2">  <span class="dt">ApplicativeDict</span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">    {<span class="ot"> _return ::</span> (forall a <span class="fu">.</span> <span class="dt">WithCode</span> (a <span class="ot">-&gt;</span> m a)),</a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="ot">      _ap     ::</span> (forall a b <span class="fu">.</span> <span class="dt">WithCode</span> (m (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> m a <span class="ot">-&gt;</span> m b))</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">    }</a></code></pre></div>
<p><code>WithCode</code> is necessary again as it will be used to generate a program so it’s necessary to know how to implement the methods.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" data-line-number="1">elimApplicative</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="ot">  ::</span> <span class="dt">SynApplicative</span> a</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">  <span class="ot">-&gt;</span> <span class="dt">ApplicativeDict</span> m</a>
<a class="sourceLine" id="cb7-4" data-line-number="4">  <span class="ot">-&gt;</span> <span class="dt">TExpQ</span> (m a)</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">elimApplicative (<span class="dt">Return</span> v) d<span class="fu">@</span><span class="dt">ApplicativeDict</span>{<span class="fu">..</span>}</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">  <span class="fu">=</span> [<span class="fu">||</span> <span class="fu">$$</span>(_code _return) <span class="fu">$$</span>(_code v) <span class="fu">||</span>]</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">elimApplicative (<span class="dt">App</span> e1 e2) d<span class="fu">@</span><span class="dt">ApplicativeDict</span>{<span class="fu">..</span>}</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">  <span class="fu">=</span> [<span class="fu">||</span> <span class="fu">$$</span>(_code _ap) <span class="fu">$$</span>(elimApplicative e1 d) <span class="fu">$$</span>(elimApplicative e2 d) <span class="fu">||</span>]</a></code></pre></div>
<p>This interpretation is very boring as it just amounts to replacing all the constructors with their implementations. However, it is exciting that we have guaranteed the removal of the overhead of the applicative abstraction.</p>
<h2 id="running-the-splice">Running the Splice</h2>
<p>Now that we’ve written two functions independently to to eliminate the two layers, they need to be combined together. This is the birth of our three-stage program.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">import</span> <span class="dt">Three</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3"><span class="ot">elim ::</span> <span class="dt">Identity</span> <span class="dt">Int</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4">elim <span class="fu">=</span> <span class="fu">$$</span>(elimApplicative <span class="fu">$$</span>(elimExpr (<span class="dt">Add</span> (<span class="dt">Val</span> <span class="dv">1</span>) (<span class="dt">Val</span> <span class="dv">2</span>))) identityDict)</a>
<a class="sourceLine" id="cb8-5" data-line-number="5"></a>
<a class="sourceLine" id="cb8-6" data-line-number="6">identityDict <span class="fu">=</span> <span class="dt">ApplicativeDict</span>{<span class="fu">..</span>}</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb8-8" data-line-number="8">    _return <span class="fu">=</span> <span class="dt">WithCode</span> <span class="dt">Identity</span> [<span class="fu">||</span> <span class="dt">Identity</span> <span class="fu">||</span>]</a>
<a class="sourceLine" id="cb8-9" data-line-number="9">    _ap <span class="fu">=</span> <span class="dt">WithCode</span> idAp [<span class="fu">||</span> idAp <span class="fu">||</span>]</a>
<a class="sourceLine" id="cb8-10" data-line-number="10"></a>
<a class="sourceLine" id="cb8-11" data-line-number="11"><span class="ot">idAp ::</span> <span class="dt">Identity</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> <span class="dt">Identity</span> a <span class="ot">-&gt;</span> <span class="dt">Identity</span> b</a>
<a class="sourceLine" id="cb8-12" data-line-number="12">idAp (<span class="dt">Identity</span> f) (<span class="dt">Identity</span> a) <span class="fu">=</span> <span class="dt">Identity</span> (f a)</a></code></pre></div>
<p><code>elim</code> is the combination of <code>elimApplicative</code> and <code>elimExpr</code>. The nested splices indicate that the program is more than two levels.</p>
<p>Using <code>-ddump-splices</code> we can have a look at the program that gets generated.</p>
<pre><code>Test.hs:10:30-59: Splicing expression
    elimExpr (Add (Val 1) (Val 2))
  ======&gt;
    ((Return ((WithCode (+)) codePlus)
        `App` Return ((WithCode 1) (liftT 1)))
       `App` Return ((WithCode 2) (liftT 2)))
Test.hs:10:11-73: Splicing expression
    elimApplicative $$(elimExpr (Add (Val 1) (Val 2))) identityDict
  ======&gt;
    (idAp ((idAp (Identity (+))) (Identity 1))) (Identity 2)</code></pre>
<p>Both steps appear in the debug output with the code which was produced at each step. Notice that we had very precise control over what code was generated and that functions like <code>idAp</code> are not inlined. In this case, the compiler will certainly inline <code>idAp</code> and so on but in general it might be useful to generate code which contains calls to <code>GHC.Exts.inline</code> to force even recursive functions to be inlined once.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In general, splitting your program up into stages is quite difficult so mechanisms like type class specialisation will be easier to achieve. In controlled situations though, staging gives you the guarantees you need.</p>
<h2 id="related-links">Related Links</h2>
<ul>
<li><a href="https://www.reddit.com/r/haskell/comments/aqkv9k/a_threestage_program_you_definitely_want_to_write/">Reddit Discussion</a></li>
<li><a href="https://github.com/mpickering/three-level">Code</a></li>
</ul>
]]></summary>
</entry>
<entry>
    <title>Implementing Nested Quotations</title>
    <link href="http://mpickering.github.io/posts/2019-01-31-nested-brackets.html" />
    <id>http://mpickering.github.io/posts/2019-01-31-nested-brackets.html</id>
    <published>2019-01-31T00:00:00Z</published>
    <updated>2019-01-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Implementing Nested Quotations </h2>
<p class="text-muted">
    Posted on January 31, 2019
    
</p>

<p>Quotation is one of the key elements of metaprogramming. Quoting an expression <code>e</code> gives us a representation of <code>e</code>.</p>
<pre><code>[| e |] :: Repr</code></pre>
<p>What this representation is depends on the metaprogramming framework and what we can do with the representation depends on the representation. The most common choice is to dissallow any inspection of the representation type relying on the other primative operation, the splice, in order to insert quoted values into larger programs.</p>
<p>The purpose of this post is to explain how to implemented nested quotations. From our previous example, quoting a term <code>e</code>, gives us a term which represents <code>e</code>. It follows that we should be allowed to nest quotations so that quoting a quotation gives us a representation of that quotation.</p>
<pre><code>[| [| 4 + 5 |] |]</code></pre>
<p>However, nesting brackets in this manner has been disallowed in Template Haskell for a number of years despite nested splices being permitted. I wondered why this restriction was in place and it seemed that <a href="https://mail.haskell.org/pipermail/ghc-devs/2019-January/016939.html">no one knew the answer</a>. It turns out, there was no technical reason and implementing nested brackets is straightforward once you think about it correctly.</p>
<!--more-->
<h2 id="template-haskell">Template Haskell</h2>
<p>We will now be concrete and talk about how these mechanisms are implemented in Template Haskell.</p>
<p>In Template Haskell the representation type of expressions is called <code>Exp</code>. It is a <a href="http://hackage.haskell.org/package/template-haskell-2.14.0.0/docs/Language-Haskell-TH-Syntax.html#t:Exp">simple ADT</a> which mirrors source Haskell programs very closely. For example quoting <code>2 + 3</code> might be represented by:</p>
<pre><code>[| 2 + 3 |] :: Exp
= InfixE (Just (LitE 5)) (VarE +) (Just (LitE 5))</code></pre>
<p>Because <code>Exp</code> is a normal data type we can define its representation in the same manner as any user defined data type. This is the purpose of the <code>Lift</code> type class which defines how to turn a value into its representation.</p>
<pre><code>class Lift t where
  lift :: t -&gt; Q Exp</code></pre>
<p>So we just need to implement <code>instance Lift (Q Exp)</code> and we’re done. To do that we implement a general instance for <code>Lift (Q a)</code> and then also an instance for <code>Exp</code>.</p>
<pre><code>instance Lift a =&gt; Lift (Q a) where
  lift qe = qe &gt;&gt;= \b&#39; -&gt; lift b&#39; &gt;&gt;= \b&#39;&#39; -&gt; return ((VarE &#39;return) `AppE` b&#39;&#39;)</code></pre>
<p>This instance collapses effects from building the inner code value into a single outer layer. In order to make the types line up correctly, we have to insert a call to <code>return</code> to the result of lifting the inner expression.</p>
<p>Instances for <code>Exp</code> and all its connected types are straightforward to define and thankfully we can use the <code>DeriveLift</code> extension in order to derive them.</p>
<pre><code>deriving instance Lift Exp
... 40 more instances
deriving instance Lift PatSynDir</code></pre>
<p>It’s now possible to write a useless program which lifts a boolean value twice before splicing it twice to get back the original program.</p>
<pre><code>-- foo = True
foo :: Bool
foo = $($(lift (lift True)))</code></pre>
<p>Running this program with <code>-ddump-splices</code> would show us that when the first splice is run, the code that is insert is the representation of <code>True</code>. After the second splice is run, this representation is turned back into <code>True</code>.</p>
<h2 id="cross-stage-persistance">Cross Stage Persistance</h2>
<p>If you use variables in a bracket the compiler has to persist their value from one stage to another so that they remain bound and bound to the correct value when we splice in the quote.</p>
<p>For example, quoting <code>x</code>, we need to remember that the <code>x</code> refers to the <code>x</code> bound to the top-level which is equal to <code>5</code>.</p>
<pre><code>x = 5

foo = [| x |]</code></pre>
<p>If we didn’t when splicing in <code>foo</code>, in another module, we would use whatever <code>x</code> was in scope or end up with an unbound reference to <code>x</code>. No good at all.</p>
<p>For a locally bound variable, we can’t already precisely know the value of the variable. We will only know it later at runtime when the function is applied.</p>
<pre><code>foo x = [| x |]</code></pre>
<p>Thus, we must know for any value that <code>x</code> can take, how we construct its representation. If we remember, that’s precisely what the <code>Lift</code> class is for. So, to correct this cross-stage reference, we replace the variable <code>x</code> with a splice (which lowers the level by one) and a call to <code>lift</code>.</p>
<pre><code>foo x = [| $(lift x) |]</code></pre>
<h3 id="nesting-brackets">Nesting Brackets</h3>
<p>The logic for persisting variables has to be extended to work with nested brackets.</p>
<pre><code>foo3 :: Lift a =&gt; a -&gt; Q Exp
foo3 x = [| [| x |] |]</code></pre>
<p>In <code>foo3</code>, <code>x</code> is used at level 2 but defined at level 0, hence we must insert two levels of splices and two levels of lifting to rectify the stages.</p>
<pre><code>foo3 :: Lift a =&gt; a -&gt; Q Exp
foo3 x = [| [| $($(lift(lift x))) |] |]</code></pre>
<p>Now with nested brackets, you can also lift variables defined in future stages.</p>
<pre><code>foo4 :: Q Exp
foo4 = [| \x -&gt; [| x |] |]</code></pre>
<p>Now <code>x</code> is defined at stage 1 and used in stage 2. So, like normal, we need to insert a lift and splice in order to realign the stages. This time, just one splice as we just need to lift it one level.</p>
<pre><code>foo4 :: Q Exp
foo4 = [| \x -&gt; [| $(lift x) |] |]</code></pre>
<h1 id="implementing-nested-brackets">Implementing Nested Brackets</h1>
<h2 id="implementing-splices">Implementing Splices</h2>
<p>After renaming a bracket, all the splices inside the bracket are moved into an associated environment.</p>
<pre><code>foo = [| $(e) |]
=&gt; [| x |]_{ x = e }</code></pre>
<p>When renaming the RHS of <code>foo</code>, we replace the splice of <code>e</code> with a new variable <code>x</code>, this is termed the “splice point” for the expression <code>e</code>. Then, a new binding is added to the environment for the bracket which says that any reference to <code>x</code> inside the bracket refers to <code>e</code>. That means when we make the representation of the code inside the bracket, occurences of <code>x</code> are replaced with <code>e</code> directly (rather than a representation of <code>x</code>) in the program.</p>
<p>The same mechanism is used for the implicit splices we create by instances of cross-stage persistence.</p>
<pre><code>qux x = [| x |]
        =&gt; [| $(lift x) |]
        =&gt; [| x&#39; |]_{ x&#39; = lift x }</code></pre>
<p>The environment is special in the sense that it connects a stage 1 variable with an expression at stage 0.</p>
<p>How is this implemented? When we see a splice we rename it and the write it to a state variable whose scope is delimited by the bracket. Once the contents of the bracket is finished being renamed we read the contents and use that as the environment.</p>
<h2 id="generalisation-to-n-levels">Generalisation to n-levels</h2>
<p>Nested splices work immediately with nested brackets. When there is a nested bracket, the expression on the inside is first floated outwards into the inner brackets environment.</p>
<pre><code>foo n = [| [| $($(n)) |] |]
      =&gt; [| [| x |]_{x=$(n)} |]
      =&gt; [| [| x |]_{x = y} |]_{y = n}</code></pre>
<p>Then it is floated again to the top-level leaving a behind a trail of bindings.</p>
<h2 id="representing-quotes">Representing Quotes</h2>
<p>Template Haskell represents renamed terms so that references remain constent after splicing. As such, our representation of a quotation in the TH AST should reflect the renamed form of brackets which includes the environment.</p>
<pre><code>data Exp = ... | BrackE [(Var, Exp)] Exp | ...</code></pre>
<p>The constructor therefore takes a list which is the environment mapping splice points to expressions and a representation of the quoted expression.</p>
<p>It is invariant that there are no splice forms in renamed syntax as they are all replaced during renaming into this environment form.</p>
<p>To represent a simple quoted expression will have an empty environment but if we also use splices then these are included as well.</p>
<pre><code>[| [| 4 |] |] =&gt; BrackE [] (representation of 4)

[| [| $(foo) |] |] =&gt; BrackE [(x, representation of foo)] (representation of x)</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>Those are the details of implementing nested brackets, if you ever need to for your own language. In the end, the patch was quite simple but it took quite a bit of thinking to work out the correct way to propagate the splices and build the correct representation.</p>
]]></summary>
</entry>
<entry>
    <title>Packaging a Haskell library for artefact evaluation using nix</title>
    <link href="http://mpickering.github.io/posts/2018-09-19-nix-artefacts.html" />
    <id>http://mpickering.github.io/posts/2018-09-19-nix-artefacts.html</id>
    <published>2018-09-19T00:00:00Z</published>
    <updated>2018-09-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Packaging a Haskell library for artefact evaluation using nix </h2>
<p class="text-muted">
    Posted on September 19, 2018
    
</p>

<p>This year I packaged two artefacts for the ICFP artefact evaluation process. This post explains the system I used to make it easy to produce the docker images using nix. I hope this documentation will be useful for anyone else submitting a Haskell library for evaluation.</p>
<p>The end result will be an <code>artefact.nix</code> file which is used to build a docker image to submit. It will be an entirely reproducible process as we will fix the versions of all the dependencies we use.</p>
<!--more-->
<h2 id="the-structure-of-the-artefact">The structure of the artefact</h2>
<p>In this example, I am going to package the artefact from the paper <a href="https://dl.acm.org/citation.cfm?id=3236780">“Generic Deriving of Generic Traversals”</a>. The artefact was a Haskell library and an executable which ran some benchmarks. The resulting artefact will be a docker image which contains:</p>
<ol type="1">
<li>The source code of the library</li>
<li>The source code for the benchmarks</li>
<li>The executable of the benchmarks</li>
<li>An environment where it is possible to compile the library and benchmarks</li>
</ol>
<p>To start with, I will assume that we have placed the source code and benchmarks code in our current directory. We will add the rest of the files</p>
<pre><code>&gt;&gt;&gt; ls
generic-lens-1.0.0.1/
benchmarks/</code></pre>
<h2 id="step-1-pinning-nixpkgs">Step 1: Pinning nixpkgs</h2>
<p>The most important step of the whole process is to “pin” our version of nixpkgs to a specific version so that anyone else trying to build the image will use the same versions of all the libraries and system dependencies.</p>
<p>Once we have established a commit of nixpkgs that our package builds with. We can use <code>nix-prefetch-git</code> in order to create <code>nixpkgs.json</code> which will provide the information about the pin.</p>
<pre><code>nix-prefetch-git --rev 651239d5ee66d6fe8e5e8c7b7a0eb54d2f4d8621 --url https://github.com/NixOS/nixpkgs.git &gt; nixpkgs.json</code></pre>
<p>Now we have a file, <code>nixpkgs.json</code> which specifies which version of nixpkgs we should use.</p>
<p>We then need to load this file. Some boilerplate, <code>nixpkgs.nix</code>, will do that for us.</p>
<pre><code>opts:
let
   hostPkgs = import &lt;nixpkgs&gt; {};
   pinnedVersion = hostPkgs.lib.importJSON ./nixpkgs.json;
   pinnedPkgs = hostPkgs.fetchFromGitHub {
     owner = &quot;NixOS&quot;;
     repo = &quot;nixpkgs&quot;;
     inherit (pinnedVersion) rev sha256;
   };
in import pinnedPkgs opts</code></pre>
<p><code>nixpkgs.nix</code> will be imported in <code>artefact.nix</code> and will determine precisely the version of all dependencies we will use.</p>
<h2 id="step-2-using-dockertools">Step 2: Using <code>dockerTools</code></h2>
<p>Now we have specified the set of dependencies we want to use we can go about starting to build our docker image. Nixpkgs provides a convenient set of functions called <code>dockerTools</code> in order to create docker images in a declarative manner. This is the start of our <code>artefact.nix</code> file.</p>
<pre><code>let
  pkgs = import ./nixpkgs.nix { };
in
with pkgs;
let
    debian = dockerTools.pullImage
      { imageName = &quot;debian&quot;
      ; imageTag = &quot;9.5&quot;
      ; sha256 = &quot;1jxci0ph7l5fh0mm66g4apq1dpcm5r7gqfpnm9hqyj7rgnh44crb&quot;; };
in
dockerTools.buildImage {
  name = &quot;generic-lens-artefact&quot;;

  fromImage = debian;

  contents = [  bashInteractive
                glibcLocales
             ];

  config = {
    Env = [&quot;LANG=en_US.UTF-8&quot;
           &quot;LOCALE_ARCHIVE=${glibcLocales}/lib/locale/locale-archive&quot;];
    WorkingDir = &quot;/programs&quot;;
  };
}</code></pre>
<p>This is the barebones example we’ll start from. We firstly import <code>nixpkgs.nix</code> which defines the package set we want to use. Our docker file will be based on <code>debian</code>, and so we use the <code>dockerTools.pullImage</code> function to get this base image. The <code>imageName</code> comes from docker hub and the <code>imageTag</code> indicates the specific tag.</p>
<p>This image is our base image when calling <code>dockerTools.buildImage</code>. For now, we add the basic packages <code>bashInteractive</code> and <code>glibcLocales</code>, in the next step we will add the specific contents that we need for our artefact.</p>
<p>Setting the <code>LANG</code> and <code>LOCALE_ARCHIVE</code> env vars is important for Haskell programs as otherwise you can run into strange encoding errors.</p>
<p>This is a complete image which can already be build with <code>nix-build artefact.nix</code>. The result will be a <code>.tar.gz</code> which can be loaded into docker and run as normal.</p>
<h2 id="step-3-including-the-artefact">Step 3: Including the artefact</h2>
<p>First we’ll deal with making the executable itself available on the image. Remember that the source code of the benchmarks, which is a normal Haskell package, is located in <code>benchmarks/</code>.</p>
<p>We need to tell nix how to build the benchmarks. The standard way to do this is to use <code>cabal2nix</code> to generate a package specification which we will pass to <code>haskellPackages.callPackage</code>.</p>
<pre><code>cabal2nix benchmarks/ &gt; benchmarks.nix</code></pre>
<p>This will produce a file which looks a bit like</p>
<pre><code>{ mkDerivation, base, criterion, deepseq, dlist, dump-core
, generic-lens, geniplate-mirror, haskell-src, lens, mtl, one-liner
, parallel, plugin, random, stdenv, syb, transformers, uniplate
, weigh
}:
mkDerivation {
  pname = &quot;benchmarks&quot;;
  version = &quot;0.1.0.0&quot;;
  src = ./benchmarks;
  isLibrary = false;
  isExecutable = true;
  executableHaskellDepends = [
    base criterion deepseq dlist dump-core generic-lens
    geniplate-mirror haskell-src lens mtl one-liner parallel plugin
    random syb transformers uniplate weigh
  ];
  license = stdenv.lib.licenses.bsd3;
}</code></pre>
<p>Now we will add the executable to the docker image. A new definition is created in the let bindings and then we add the executable to the <code>contents</code> of the image.</p>
<pre><code>run-benchmarks = haskellPackages.callPackage ./benchmarks.nix {};</code></pre>
<p>So now our <code>contents</code> section will look like:</p>
<pre><code>  contents = [  bashInteractive
                glibcLocales
                run-benchmarks
                ];</code></pre>
<p>When we build this image, the executable will be available on the path by default. In our case, the user will type <code>bench</code> and it will run the benchmarks.</p>
<h2 id="step-4-including-the-source-files">Step 4: Including the source files</h2>
<p>The next step is to add the source files to the image. To do this we use the <code>runCommand</code> script to make a simple derivation which copies some files into the right place.</p>
<pre><code>benchmarks-raw = ./benchmarks;
benchmarks =
  runCommand &quot;benchmarks&quot; {} &#39;&#39;
  mkdir -p $out/programs/benchmarks
  cp -r ${benchmarks-raw}/* $out/programs/benchmarks
&#39;&#39;;</code></pre>
<p>All the derivation does is copy the directory into the nix store at a specific path. We then just add this to the <code>contents</code> list again and also do the same for the library itself and the README.</p>
<pre><code>  contents = [  bashInteractive
                glibcLocales
                run-benchmarks
                benchmarks
                readme
                library];</code></pre>
<p>Now once we build the docker image, we’ll have the executable <code>bench</code> available and also a file called <code>README</code> and two folders containing the library code and benchmarks code.</p>
<h2 id="step-5-an-environment-to-build-the-source-code">Step 5: An environment to build the source code</h2>
<p>Finally, we need to do two more things to make it possible to build the source programs in the container.</p>
<p>Including <code>cabal-install</code> in the contents is the first so that we can use <code>cabal</code> in the container.</p>
<pre><code>  contents = [  bashInteractive
                glibcLocales
                run-benchmarks
                benchmarks
                readme
                library
                cabal-install ];</code></pre>
<p>The second is much less obvious, we need to make sure that the necessary dependencies are already installed in the environment so that someone can just use <code>cabal build</code> in order to build the package. The way to achieve this is to modify the <code>benchmarks.nix</code> file and change <code>isLibrary</code> to <code>true</code>.</p>
<pre><code>-  isLibrary = false;
+  isLibrary = true;</code></pre>
<p>This means that all the build inputs for the benchmarks are propagated to the container so all the dependencies for the benchmarks will be available to rebuild them again.</p>
<h2 id="complete-artefact.nix">Complete <code>artefact.nix</code></h2>
<p>Here’s the complete <code>artefact.nix</code> that we ended up with. We also generated <code>nixpkgs.json</code>, <code>nixpkgs.nix</code> and <code>benchmarks.nix</code> along the way.</p>
<pre><code>let
  pkgs = import ./nixpkgs.nix {};
in
with pkgs;
let
    debian = dockerTools.pullImage
      { imageName = &quot;debian&quot;
      ; imageTag = &quot;9.5&quot;
      ; sha256 = &quot;1y4k42ljf6nqxfq7glq3ibfaqsq8va6w9nrhghgfj50w36bq1fg5&quot;; };

    benchmarks-raw = ./benchmarks;
    benchmarks =
      runCommand &quot;benchmarks&quot; {} &#39;&#39;
        mkdir -p $out/programs/benchmarks
        cp -r ${benchmarks-raw}/* $out/programs/benchmarks
      &#39;&#39;;

    library-raw = ./generic-lens-1.0.0.1;
    library =
      runCommand &quot;benchmarks&quot; {} &#39;&#39;
        mkdir -p $out/programs/library
        cp -r ${library-raw}/* $out/programs/library
      &#39;&#39;;

    readme-raw = ./README;
    readme =
      runCommand &quot;readme&quot; {} &#39;&#39;
        mkdir -p $out/programs
        cp ${readme-raw} $out/programs/README
      &#39;&#39;;

    run-benchmarks = haskellPackages.callPackage ./benchmarks.nix {};

in
dockerTools.buildImage {
  name = &quot;generic-lens-artefact&quot;;


  fromImage = debian;

  contents = [  bashInteractive
                cabal-install
                glibcLocales
                run-benchmarks
                benchmarks
                readme
                library];

  config = {
    Env = [&quot;LANG=en_US.UTF-8&quot;
           &quot;LOCALE_ARCHIVE=${glibcLocales}/lib/locale/locale-archive&quot;];
    WorkingDir = &quot;/programs&quot;;
  };
}</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Hopefully this tutorial will be useful for anyone having to package a Haskell library in future. Each artefact is different so you’ll probably have to modify some of the steps in order to make it work perfectly for you. It’s also possible that the <code>dockerTools</code> interface will change but it should be possible to modify the examples here to adapt to any minor changes. If you’re already using nix, you probably know what you’re doing anyway.</p>
<h2 id="related-links">Related Links</h2>
<ul>
<li><a href="https://vaibhavsagar.com/blog/2018/05/27/quick-easy-nixpkgs-pinning/">Quick and Easy Nixpkgs Pinning - Vaibhav Sagar</a></li>
<li><a href="https://nixos.org/nixpkgs/manual/#sec-pkgs-dockerTools"><code>dockerTools</code> documentation</a></li>
<li><a href="https://www.software.ac.uk/blog/2017-10-05-reproducible-environments-nix">Reproducible Environments With Nix - Blair Archibald</a></li>
<li><a href="https://www.reddit.com/r/haskell/comments/9h6xh2/packaging_a_haskell_library_for_artefact/">Reddit comments</a></li>
</ul>
]]></summary>
</entry>
<entry>
    <title>Using funflow to cache a nix based workflow</title>
    <link href="http://mpickering.github.io/posts/2018-09-12-funflow-nix.html" />
    <id>http://mpickering.github.io/posts/2018-09-12-funflow-nix.html</id>
    <published>2018-09-12T00:00:00Z</published>
    <updated>2018-09-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2> Using funflow to cache a nix based workflow </h2>
<p class="text-muted">
    Posted on September 12, 2018
    
</p>

<p>My latest project has been to plot a <a href="http://mpickering.github.io/maps.html">map of orienteering maps</a> in the UK. This post explains the technical aspects behind the project and primarily the use of <a href="https://hackage.haskell.org/package/funflow"><code>funflow</code></a> to turn my assortment of scripts into into a resumable workflow.</p>
<p>There was nothing wrong with my ad-hoc python and bash scripts but they downloaded and regenerated the whole output every time. The whole generation takes about 2 hours so it’s desirable to only recompute the necessary portions. This is where <code>funflow</code> comes in, by stringing together these scripts in their DSL, you get caching for free. The workflow is also highly parallelisable so in the future I could distribute the work across multiple machines if necessary.</p>
<p>The code for the project can be found <a href="https://github.com/mpickering/rg-map">here</a>.</p>
<p><img src="https://i.imgur.com/NVLui01.png" style="width: 50%; margin: auto; display: block" /></p>
<!--more-->
<h2 id="funflow"><code>funflow</code></h2>
<p>There are already two blog posts introducing the concepts of <code>funflow</code>.</p>
<ol type="1">
<li><a href="https://www.tweag.io/posts/2018-04-25-funflow.html">Funflow: Typed Resumable Workflows</a></li>
<li><a href="https://www.tweag.io/posts/2018-07-10-funflow-make.html">Funflow Example: Emulating Make</a></li>
</ol>
<p>The main idea is that you specify your workflow (usually a sequence of external scripts) using a DSL and then <code>funflow</code> will automatically cache and schedule the steps.</p>
<p>My primary motivation for using <code>funflow</code> was the automatic caching. The store is content addressed which means that the location for each file in the store depends on its contents. <code>funflow</code> performs two different types of caching.</p>
<ol type="1">
<li>Input-based caching: A flow will only be executed once for each set of inputs.</li>
<li>Output-based caching: If multiple different steps produce the same output then it will only be stored once in the store. Further steps will not be recomputed.</li>
</ol>
<p>The lack of output-based caching is one of the big missing features of nix which makes it unsuitable for this task. A content-addressed store where the address depends on the contents of the file is sometimes known as an <strong>intensional</strong> store. Nix’s store model is <strong>extensional</strong> as the store hash only depends on the inputs to the build.</p>
<p>An intensional store relies on the program producing deterministic output hashes. It can be quite difficult to track down why a step is not being cached when you are relying on the output’s being identified in the store.</p>
<h2 id="high-level-architecture">High-level architecture</h2>
<p>There are two outputs to the project.</p>
<ol type="1">
<li>A folder of map tiles rendered at different resolutions.</li>
<li>A HTML page which contains the javascript to display the map and markers.</li>
</ol>
<p>This folder is then uploaded to online storage and served as a static site.</p>
<p>The processing pipeline is as follows:</p>
<ol type="1">
<li>Find all the maps with location information from <a href="http://www.routegadget.co.uk/">routegadget.co.uk</a>.</li>
<li>Download the metainformation and map image for each map.</li>
<li>Convert the maps to a common image format.</li>
<li>Reproject the maps to remove any rotation.</li>
<li>Merge overlapping maps into groups.</li>
<li>Generate tiles at all the different resolutions.</li>
<li>Combine all the tiles groups together.</li>
<li>Generate the website with the map and location markers.</li>
</ol>
<p>As can be seen, the workflow is firstly highly parallelisable as much of the processing pipeline happens independently of other steps. However, the main goal is to avoid computing the tiles as much as possible as this is the step which takes by far the longest. At the time of writing there are about 500 maps to process. In general, there are about 5-10 maps added each week. Only recomputing the changed portions of the map saves a lot of time.</p>
<h2 id="implementation-using-funflow">Implementation using <code>funflow</code></h2>
<p>In theory, this is a perfect application for <code>funflow</code> but in order to achieve the perfect caching behaviour I had to rearchitecture several parts of the application.</p>
<h3 id="using-nix-scripts">Using nix scripts</h3>
<p>The recommended way to use <code>funflow</code> is to run each step of the flow in a docker container. I didn’t want to do this was my scripts already declared the correct environment to run in by using the <a href="http://iam.travishartwell.net/2015/06/17/nix-shell-shebang/"><code>nix-shell</code> shebang</a>.</p>
<pre><code>#! /usr/bin/env nix-shell
#! nix-shell -i bash -p gdal</code></pre>
<p>By placing these two lines at the top of the file, the script will be run using the <code>bash</code> interpreter with the <code>gdal</code> package available. This is more lightweight and flexible than using a docker image as I don’t have to regenerate a new docker image any time I make a change.</p>
<p>However, there is no native support for running these kinds of scripts built into <code>funflow</code>. It was easy enough to define my own function in order to run these kinds of scripts using the <code>external'</code> primitive.</p>
<p><code>nixScript</code> takes a boolean parameter indicating whether the script is pure and should be cached. The name of the script to run, the names of any files the script depends on and finally a function which supplies any additional arguments to the script.</p>
<pre><code>nixScriptX :: ArrowFlow eff ex arr =&gt; Bool
                                   -&gt; Path Rel File
                                   -&gt; [Path Rel File]
                                   -&gt; (a -&gt; [Param])
                                   -&gt; arr (Content Dir, a) CS.Item
nixScriptX impure script scripts params = proc (scriptDir, a) -&gt; do
  env &lt;- mergeFiles -&lt; absScripts scriptDir
  external&#39; props (\(s, args) -&gt; ExternalTask
        { _etCommand = &quot;perl&quot;
        , _etParams = contentParam (s ^&lt;/&gt; script) : params args
        , _etWriteToStdOut = NoOutputCapture
        , _etEnv = [(&quot;NIX_PATH&quot;, envParam &quot;NIX_PATH&quot;)] }) -&lt; (env, a)
  where
    props = def { ep_impure = impure }
    absScripts sd = map (sd ^&lt;/&gt;) (script : scripts)</code></pre>
<p>The use of <code>perl</code> as the command relies on <a href="http://perldoc.perl.org/perlrun.html">the behaviour</a> of <code>perl</code> that it will execute the <code>#!</code> line if it does not contain the word “perl”. Yes, this <a href="https://askubuntu.com/questions/850384/is-there-a-command-for-running-a-script-according-to-its-shebang-line/850387">is dirty</a>.</p>
<p>It would be desirable to set <code>NIX_PATH</code> to a fixed <code>nixpkgs</code> checkout by passing a tarball directly but this worked for now.</p>
<p>All the steps are then defined in terms of <code>nixScriptX</code> indirectly as two helper functions are defined for the two cases of a pure or impure scripts.</p>
<pre><code>nixScript = nixScriptX False
impureNixScript = nixScriptX True</code></pre>
<h2 id="step-1---finding-the-map-information">Step 1 - Finding the map information</h2>
<p>Now to the nitty gritty details.</p>
<p>Firstly, I had to decouple the processing of finding the map metainformation from downloading the image. Otherwise, I would end up doing a lot of redundant work downloading images multiple times.</p>
<p>The python script <code>scraper.py</code> executes a selenium driver to extract the map information. For each map, the metainformation is serialised to its own file in the output directory.</p>
<pre><code>scrape = impureNixScript [relfile|scraper.py|] [[relfile|shell.nix|]]
          (\() -&gt; [ outParam ])</code></pre>
<p>This step is marked as impure as we have to run it every time the flow runs to work out if we need to perform any more work.</p>
<p>It is important that the filename of the serialised information is the same if the content of the file is the same. Otherwise, <code>funflow</code> will calculate a different hash for the file. As such, we compute our own hash of the metainformation for the name the serialised file.</p>
<p>In the end the output directory looks like:</p>
<pre><code>9442c7eaa81f82f7e9889f6ee8382e8d047df76db2d5f6a6983d1c82399a2698.pickle
5e7e6994db565126a942d66a9435454d8b55cd7d3023dd37f64eca7bbb46df1f.pickle
...</code></pre>
<h3 id="gotcha-1-using-listdircontents-defeats-caching">Gotcha 1: Using <code>listDirContents</code> defeats caching</h3>
<p>Now that we have a directory containing all the metainformation, we want to split it up and then execute the fetching, converting and warping in parallel for all the images. My first attempt was</p>
<pre><code>meta_dir &lt;- step All &lt;&lt;&lt; scrape -&lt; (script_dir, ())
keys &lt;- listDirContents -&lt; meta_dir</code></pre>
<p>but this did not work and even if the keys remained the same, the images would be refetched. The problem was <a href="https://hackage.haskell.org/package/funflow-1.3.2/docs/Control-Funflow-Steps.html#v:listDirContents"><code>listDirContents</code></a> does not have the correct caching behaviour.</p>
<p><code>listDirContents</code> takes a <code>Content Dir</code> and returns a <code>[Content File]</code> as required but the <code>[Content File]</code> are pointers into places into the <code>Content Dir</code>. This means that if the location of <code>Content Dir</code> changes (if there are any changes or new additions to any files in the directory) then the location of <em>all</em> the <code>[Content File]</code> will also be changed. This means the next stage of recompilation will be triggered despite being unnecessary.</p>
<p>Instead, we have to put each file in the directory into its own store location so that the its location depends only on itself rather than the other contents of the directory. I defined the <code>splitDir</code> combinator in order to do this.</p>
<pre><code>splitDir :: ArrowFlow eff ex arr =&gt; arr (Content Dir) ([Content File])
splitDir = proc dir -&gt; do
  (_, fs) &lt;- listDirContents -&lt; dir
  mapA reifyFile -&lt; fs


-- Put a file, which might be a pointer into a dir, into its own store
-- location.
reifyFile :: ArrowFlow eff ex arr =&gt; arr (Content File) (Content File)
reifyFile = proc f -&gt; do
  file &lt;- getFromStore return -&lt; f
  putInStoreAt (\d fn -&gt; copyFile fn d) -&lt; (file, CS.contentFilename f)</code></pre>
<p>It could be improved by using a hardlink rather than <code>copyFile</code>.</p>
<h2 id="step-2-download-convert-and-warp">Step 2: Download, convert and warp</h2>
<p>Now we have split the metainformation up into individual components we have to download, convert and warp the map files.</p>
<p>We define three flows to do this which correspond to three different scripts.</p>
<pre><code>fetch = nixScript [relfile|fetch.py|] [[relfile|shell.nix|]]
          (\metadata -&gt; [ outParam, contentParam metadata ])

convertToGif = nixScript [relfile|convert_gif|] []
                (\dir -&gt; [ pathParam (IPItem dir), outParam ])

warp = nixScript [relfile|do_warp|] []
        (\dir -&gt; [ pathParam (IPItem dir), outParam ])</code></pre>
<p>Each script takes an individual input file and produces output in a directory specified by <code>funflow</code>.</p>
<p><code>fetch.py</code> is a python script whilst <code>convert_gif</code> and <code>do_warp</code> are bash scripts. We can treat them uniformly because of the <code>nix-shell</code> shebang.</p>
<p>These steps are all cached by default because they are external processes.</p>
<h2 id="step-3-merge-the-images-together">Step 3: Merge the images together</h2>
<p>In order to get a good looking result, we need to group together the processed images into groups of overlapping images. This time we will use a python script again invoked in a similar manner. The output is a directory of files which specify the groups, remember:</p>
<ol type="1">
<li>We have to be careful naming the files so that the names remain stable across compilation. In my original program the names were supplied by a counter but now they are the hash of the files which were used to create the group.</li>
<li>We have to use <code>splitDir</code> after creating the output to put each group file into it’s own store location so the next recompilation step will work.</li>
</ol>
<pre><code>mergeRasters = nixScript [relfile|merge-rasters.py|] [[relfile|merge-rasters.nix|]]
                (\rs -&gt; outParam : map contentParam rs )</code></pre>
<p>This command also relies on <code>merge-rasters.nix</code> which sets up the correct python environment to run the script.</p>
<h3 id="gotcha-2-mergedirs-can-also-defeat-caching">Gotcha 2: <code>mergeDirs</code> can also defeat caching</h3>
<p>The original implementation of this used <code>mergeDirs :: arr [Content File] (Content Dir)</code> in order to group together the files and pass a single directory to <code>merge-rasters.py</code>.</p>
<p>However, this suffers a similar problem to <code>listDirContents</code> as <code>mergeDirs</code> will create a new content store entry which contains all the files in the merge directories. The hash of this store location will then depend on the whole contents of the directory. In this case these file paths ended up in the output so it would cause the next steps to recompile even if nothing had changed.</p>
<p>In this case, we would prefer a “logical” group which groups the files together with a stable filename which wouldn’t affect caching.</p>
<p>The workaround for now was to use <code>splitDir</code> again to put each processed image into its own storage path and then pass each filename individually to <code>merge-rasters.py</code> rather than a directory as before.</p>
<h2 id="step-4-making-the-tiles">Step 4: Making the tiles</h2>
<p>Making the tiles is another straightforward step which takes each of the groups and makes the necessary tiles for that group.</p>
<pre><code>makeTiles = nixScript [relfile|make_tiles|] [] (\dir -&gt; [ contentParam dir, outParam, textParam &quot;16&quot; ])</code></pre>
<h3 id="gotcha-3-mergedirs-doesnt-merge-duplicate-files">Gotcha 3: <code>mergeDirs</code> doesn’t merge duplicate files</h3>
<p>Once we have made all the tiles we need to merge them all together. This is safe as we already ensured that they didn’t overlap each other. The problem is that <code>mergeDirs</code> will not merge duplicate files. The <code>make_tiles</code> step creates some unnecessary files which we don’t need but would cause <code>mergeDirs</code> to fail as they are contained in the output of each directory.</p>
<p>The solution was to write my own version of <code>mergeDirs</code> which checks to see whether a file already exists before trying to merge it. It would be more hygienic to ensure that the directories I was trying to merge were properly distinct but this worked well for this use case.</p>
<h2 id="step-5-creating-the-static-site">Step 5: Creating the static site</h2>
<p>Our final script is a python script which creates the static site displaying all the markers and the map tiles. It takes the output of processing all the images and the metainformation to produce a single html file.</p>
<pre><code>leaflet &lt;- step All &lt;&lt;&lt; makeLeaflet -&lt; ( script_dir, (merge_dir, meta_dir))</code></pre>
<p>The final step then merges together the static page and all the tiles. This is a nice bundle we can directly upload and serve our static site.</p>
<pre><code>mergeDirs -&lt; [leaflet, tiles]</code></pre>
<h2 id="putting-it-all-together">Putting it all together</h2>
<p>The complete flow is shown below:</p>
<pre><code>mainFlow :: SimpleFlow () (Content Dir)
mainFlow = proc () -&gt; do
  cwd &lt;- stepIO (const getCurrentDir) -&lt; ()
  script_dir &lt;- copyDirToStore -&lt; (DirectoryContent (cwd &lt;/&gt; [reldir|scripts/|]), Nothing)

	# Step 1
  meta_dir &lt;- step All &lt;&lt;&lt; scrape -&lt; (script_dir, ())
  keys &lt;- splitDir -&lt; meta_dir
	# Step 2
  maps &lt;- mapA (fetch) -&lt; [( script_dir, event) | event &lt;- keys]
  mapJpgs &lt;- mapA convertToGif -&lt; [(script_dir, m) | m &lt;- maps]
  merge_dir &lt;- mergeDirs&#39; &lt;&lt;&lt; mapA (step All) &lt;&lt;&lt; mapA warp -&lt; [(script_dir, jpg) | jpg &lt;- mapJpgs ]
  toMerge &lt;- splitDir -&lt; merge_dir
	# Step 3
  vrt_dir &lt;- step All &lt;&lt;&lt; mergeRasters -&lt; (script_dir, toMerge)
  merged_vrts &lt;- splitDir -&lt; vrt_dir
	# Step 4
  tiles &lt;- mergeDirs&#39; &lt;&lt;&lt; mapA (step All) &lt;&lt;&lt; mapA makeTiles -&lt; [(script_dir, vrt) | vrt &lt;- merged_vrts]

	# Step 5
  leaflet &lt;- step All &lt;&lt;&lt; makeLeaflet -&lt; ( script_dir, (merge_dir, meta_dir))

  mergeDirs -&lt; [leaflet, tiles]</code></pre>
<p>Once all the kinks are ironed out – it’s quite short but a very powerful specification which avoids a lot of redundant work being carried out.</p>
<h3 id="gotcha-4-copydirtostore-can-defeat-caching">Gotcha 4: <code>copyDirToStore</code> can defeat caching</h3>
<p>Using <code>copyDirToStore</code> seems much more convenient than copying each script into the store manually but it can again have confusing caching behaviour. The hash of the store location for <code>script_dir</code> depends on the whole <code>script_dir</code> directory. If you change any file in the directory then the hash of it will change. This means that all steps will recompile if you modify any script!</p>
<p>This is the reason for the <code>mergeFiles</code> call in <code>nixScriptX</code>. <code>mergeFiles</code> will take the necessary files from <code>script_dir</code> and put them into their own store directory. The hash of this directory will only depend on the files necessary for that step.</p>
<h2 id="running-the-flow">Running the flow</h2>
<p>The flow is run with the simple local runner. We pass in a location for the local store to the runner which is just a local directory in this case. The library has support for more complicated runners but I haven’t explored using those yet.</p>
<pre><code>main :: IO ()
main = do
    cwd &lt;- getCurrentDir
    r &lt;- withSimpleLocalRunner (cwd &lt;/&gt; [reldir|funflow-example/store|]) $ \run -&gt;
      run (mainFlow &gt;&gt;&gt; storePath) ()
    case r of
      Left err -&gt;
        putStrLn $ &quot;FAILED: &quot; ++ displayException err
      Right out -&gt; do
        putStrLn $ &quot;SUCCESS&quot;
        putStrLn $ toFilePath out</code></pre>
<h3 id="displaying-the-outpath">Displaying the outpath</h3>
<p>A nice feature of <code>nix-build</code> is that it displays the path of the final output in the nix store once the build has finished. This is possible to replicate using <code>funflow</code> after defining your own combinator. It would be good to put this in the standard library.</p>
<pre><code>storePath :: ArrowFlow eff ex arr =&gt; arr (Content Dir) (Path Abs Dir)
storePath = getFromStore return</code></pre>
<p>It means that we can run our flow and deploy the site in a single command given we have a script which performs the deployment given an output path.</p>
<p>Mine looks a bit like:</p>
<pre><code>#! /usr/bin/env nix-shell
#! nix-shell -i bash -p awscli
if [[ $# -eq 0 ]] ; then
     echo &#39;Must pass output directory&#39;
     exit 1
fi
aws s3 sync $1 s3://&lt;bucket-name&gt;</code></pre>
<p>Putting them together:</p>
<pre><code>cabal new-run | ./upload-s3</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Once everything is set up properly, <code>funflow</code> is a joy to use. It abstracts beautifully away from the annoying problems of scheduling and caching leaving the core logic visible. An unfortunate consequence of the intensional store model is that debugging why a build step is not being cached can be very time consuming and fiddly. When I explain the problems I faced, they are obvious but each one required careful thought and reading the source code to understand the intricacies of each of the different operations.</p>
<p>It was also very pleasant to combine using <code>nix</code> and <code>funflow</code> rather than the suggested <code>docker</code> support.</p>
<h2 id="related-links">Related Links</h2>
<ul>
<li><a href="http://mpickering.github.io/maps.html">Map of orienteering maps</a></li>
<li><a href="https://github.com/mpickering/rg-map">Source code</a></li>
<li><a href="https://www.reddit.com/r/haskell/comments/9f7kq9/using_funflow_to_cache_a_nix_based_workflow/">Reddit comments</a></li>
</ul>
]]></summary>
</entry>

</feed>
